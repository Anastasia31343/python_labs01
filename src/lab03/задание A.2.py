import re

def tokenize(text: str) -> list[str]:
    if not text:
        return []
    
    pattern = r'\b[\w]+(?:-[\w]+)*\b'
    tokens = re.findall(pattern, text)
    
    return tokens

result1 = tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")
print(result1)
    
result2 = tokenize("hello,world!!!")
print(result2)
    
result3 = tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ")
print(result3)
    
result4 = tokenize("2025 –≥–æ–¥")
print(result4)
    
result5 = tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ")
print(result5)